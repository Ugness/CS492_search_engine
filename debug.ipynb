{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from data.dataloader import get_tokenizer\n",
    "from model.model import GPT\n",
    "from data.utils import Trie\n",
    "import pickle\n",
    "import numpy as np \n",
    "import torch\n",
    "\n",
    "from data.dataloader import SearchData\n",
    "from torch.utils.data import DataLoader\n",
    "from model.env import Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "actor = GPT(tokenizer=tokenizer, vocab_size=len(tokenizer), block_size=100).cuda()\n",
    "reader = GPT(tokenizer=tokenizer, vocab_size=len(tokenizer), block_size=100).cuda()\n",
    "optimizer = torch.optim.Adam(actor.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/trie.pkl', 'rb') as f:\n",
    "    trie = pickle.load(f)\n",
    "with open('data/dataset.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "    _, database = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SearchData('data/train.pkl', tokenizer)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env(reader, database, trie, tokenizer, max_len=100).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_policy_gradient(\n",
    "        act_model,\n",
    "        act_opt,\n",
    "        train_loader,\n",
    "        env,\n",
    "        gamma=0.99,\n",
    "        temperature=1.0,\n",
    "        top_k=None,\n",
    "        top_p=None,\n",
    "        writer=None):\n",
    "\n",
    "    pbar = tqdm(train_loader)\n",
    "    for i, (data, indices, gt_y) in enumerate(train_loader):\n",
    "        data, indices, gt_y = data.cuda(), indices.cuda(), gt_y.cuda()\n",
    "        obs, indices, current_nodes = env.reset(data, indices, gt_y)\n",
    "        B = len(obs)\n",
    "        done = torch.zeros(len(data), dtype=torch.bool, device=obs.device, requires_grad=False)\n",
    "        episode_reward = torch.zeros(len(data), device=data.device, requires_grad=False)\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        while not done.all():\n",
    "            action_logits, action_probs, log_prob, action = act_model.predict(obs, indices, current_nodes, env.trie,\n",
    "                                                                              temperature, top_k, top_p)\n",
    "            obs = obs.detach().clone()\n",
    "            indices = indices.detach().clone()\n",
    "            with torch.no_grad():\n",
    "                obs, indices, current_nodes, reward, done, _ = env.step(obs, indices, current_nodes, action, done)\n",
    "            episode_reward += reward\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward.detach())\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.stack(returns, -1) # (B, T)\n",
    "        log_probs = torch.stack(log_probs, -1)\n",
    "        advantages = returns - returns.mean(-1, keepdim=True)\n",
    "        policy_loss = -(log_probs * advantages.detach()).mean()\n",
    "        act_opt.zero_grad()\n",
    "        loss = policy_loss\n",
    "        loss.backward()\n",
    "        act_opt.step()\n",
    "        if writer is not None:\n",
    "            writer.add_scalar('Loss/policy_loss', policy_loss.item(), i)\n",
    "            writer.add_scalar('Reward/episode_reward', episode_reward, i)\n",
    "\n",
    "        pbar.update()\n",
    "        pbar.set_description(f\"Episode {i+1}: reward={episode_reward.mean().item()}, policy_loss={policy_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 69: reward=-2.9510374069213867, policy_loss=0.03739551827311516:   3%|â–Ž         | 69/2500 [01:08<39:27,  1.03it/s] "
     ]
    }
   ],
   "source": [
    "train_policy_gradient(actor, optimizer, loader, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
